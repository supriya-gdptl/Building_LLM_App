{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Retrieval Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q --upgrade google-generativeai langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langchain chromadb langchain-community \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weave version 0.51.12 is available!  To upgrade, please run:\n",
      " $ pip install weave --upgrade\n",
      "Logged in as Weights & Biases user: supriyagdptl.\n",
      "View Weave data at https://wandb.ai/supriyagdptl/gemini-weave3_retrieval/weave\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weave.trace.weave_client.WeaveClient at 0x12eb19780>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "weave.init(\"gemini-weave3_retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your Google Gemini API key from: https://aistudio.google.com/app/apikey\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_GEMINI_API_KEY = getpass(\"Paste your Google Gemini API key from: https://aistudio.google.com/app/apikey\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Pro',\n",
      "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
      "      input_token_limit=2000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=64)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"models/gemini-1.5-pro\"\n",
    "model_info = genai.get_model(model_name)\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "\n",
    "[LangChain](https://docs.langchain.com/docs/) is a framework for developing applications powered by language models. We will use some of its features in the code below. Let's start by configuring W&B tracing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a single line of code to start tracing langchain with W&B\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "\n",
    "# wandb documentation to configure wandb using env variables\n",
    "# https://docs.wandb.ai/guides/track/advanced/environment-variables\n",
    "# here we are configuring the wandb project name\n",
    "os.environ[\"WANDB_PROJECT\"] = \"llmapps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing documents\n",
    "\n",
    "We will use a small sample of markdown documents in this notebook. Let's find them and make sure we can stuff them into the prompt. That means they may need to be chunked and not exceed some number of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "def find_md_files(directory):\n",
    "    \"Find all markdown files in a directory and return a LangChain Document\"\n",
    "    dl = DirectoryLoader(directory, \"**/*.md\")\n",
    "    return dl.load()\n",
    "\n",
    "documents = find_md_files('docs_sample/')\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "md_text_splitter = MarkdownTextSplitter(chunk_size=1000)\n",
    "document_sections = md_text_splitter.split_documents(documents)\n",
    "len(document_sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at the first section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'docs_sample/lightning.md'}, page_content=\"import Tabs from '@theme/Tabs';\\n\\nimport TabItem from '@theme/TabItem';\\n\\n# PyTorch Lightning\\n\\n[! [Open In Colab](https://colab.research.google.com/assets/colab\\n\\nbadge.svg)](https://wandb.me/lightning)\\n\\nPyTorch Lightning provides a lightweight wrapper for organizing your PyTorch code and easily adding advanced features such as distributed training and 16-bit precision. W&B provides a lightweight wrapper for logging your ML experiments. But you don't need to combine the two yourself: Weights & Biases is incorporated directly into the PyTorch Lightning library via the [**`WandbLogger`**](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch\\\\_lightning.loggers.WandbLogger.html#pytorch\\\\_lightning.loggers.WandbLogger).\\n\\n## ⚡ Get going lightning-fast with just two lines.\\n\\n```python\\n\\nfrom pytorch_lightning.loggers import WandbLogger\\n\\nfrom pytorch_lightning import Trainer\\n\\nwandb_logger = WandbLogger()\\n\\ntrainer = Trainer(logger=wandb_logger)\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_sections[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Let's now use embeddings with a vector database retriever to find relevant documents for a query. \n",
    "\n",
    "\n",
    "[Reference](https://github.com/google/generative-ai-docs/blob/main/examples/gemini/python/vectordb_with_chroma/vectordb_with_chroma.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'embedContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "  def __call__(self, input: Documents) -> Embeddings:\n",
    "    model = 'models/embedding-001'\n",
    "    title = \"Custom query\"\n",
    "    return genai.embed_content(model=model,\n",
    "                                content=input,\n",
    "                                task_type=\"retrieval_document\",\n",
    "                                title=title)[\"embedding\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [sections.page_content for sections in document_sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(name=\"wandb\", embedding_function=GeminiEmbeddingFunction())\n",
    "\n",
    "for i, d in enumerate(docs):\n",
    "    db.add(\n",
    "      documents=d,\n",
    "      ids=str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a retriever from the db now, we can pass the `k` param to get the most relevant sections from the similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['25', '26', '93']],\n",
       " 'distances': [[0.27467256784439087, 0.3961881697177887, 0.4060346782207489]],\n",
       " 'metadatas': [[None, None, None]],\n",
       " 'embeddings': None,\n",
       " 'documents': [[\"description: Collaborate and share W&B Reports with peers, co-workers, and your team.\\n\\nCollaborate on reports\\n\\nOnce you have saved a report, you can select the Share button to collaborate. A draft copy of the report is created when you select the Edit button. Draft reports auto-save. Select Save to report to publish your changes to the shared report.\\n\\nA warning notification will appear if an edit conflict occurs. This can occur if you and another collaborator edit the same report at the same time. The warning notification will guide you to resolve potential edit conflicts.\\n\\nComment on reports\\n\\nClick the comment button on a panel in a report to add a comment directly to that panel.\\n\\nWho can edit and share reports?\\n\\nReports that are created within an individual's private project is only visible to that user. The user can share their project to a team or to the public.\",\n",
       "   \"Who can edit and share reports?\\n\\nReports that are created within an individual's private project is only visible to that user. The user can share their project to a team or to the public.\\n\\nOn team projects, both the administrator, or member who created the report, can toggle permissions between edit or view access for other team members. Team members can share reports.\\n\\nTo share a report, select the Share button on the upper right hand corner. You can either provide an email account or copy the magic link. Users invited by email will need to log into Weights & Biases to view the report. Users who are given a magic link to not need to log into Weights & Biases to view the report.\\n\\nShared reports are view-only.\",\n",
       "   'description: >- Collaborate with your colleagues, share results, and track all the experiments across your team\\n\\nTeams\\n\\nUse W&B Teams as a central workspace for your ML team to build better models faster.\\n\\nTrack all the experiments your team has tried so you never duplicate work.\\n\\nSave and reproduce previously trained models.\\n\\nShare progress and results with your boss and collaborators.\\n\\nCatch regressions and immediately get alerted when performance drops.\\n\\nBenchmark model performance and compare model versions.\\n\\nCreate a collaborative team\\n\\nSign up or log in to your free W&B account.\\n\\nClick Invite Team in the navigation bar.\\n\\nCreate your team and invite collaborators.\\n\\n:::info Note: Only the admin of an organization can create a new team. :::\\n\\nInvite team members']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents', 'distances']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How can I share my W&B report with my team members in a public W&B project?\"\n",
    "db.query(query_texts=[query], n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = db.query(query_texts=[query], n_results=3)[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"description: Collaborate and share W&B Reports with peers, co-workers, and your team.\\n\\nCollaborate on reports\\n\\nOnce you have saved a report, you can select the Share button to collaborate. A draft copy of the report is created when you select the Edit button. Draft reports auto-save. Select Save to report to publish your changes to the shared report.\\n\\nA warning notification will appear if an edit conflict occurs. This can occur if you and another collaborator edit the same report at the same time. The warning notification will guide you to resolve potential edit conflicts.\\n\\nComment on reports\\n\\nClick the comment button on a panel in a report to add a comment directly to that panel.\\n\\nWho can edit and share reports?\\n\\nReports that are created within an individual's private project is only visible to that user. The user can share their project to a team or to the public.\",\n",
       " \"Who can edit and share reports?\\n\\nReports that are created within an individual's private project is only visible to that user. The user can share their project to a team or to the public.\\n\\nOn team projects, both the administrator, or member who created the report, can toggle permissions between edit or view access for other team members. Team members can share reports.\\n\\nTo share a report, select the Share button on the upper right hand corner. You can either provide an email account or copy the magic link. Users invited by email will need to log into Weights & Biases to view the report. Users who are given a magic link to not need to log into Weights & Biases to view the report.\\n\\nShared reports are view-only.\",\n",
       " 'description: >- Collaborate with your colleagues, share results, and track all the experiments across your team\\n\\nTeams\\n\\nUse W&B Teams as a central workspace for your ML team to build better models faster.\\n\\nTrack all the experiments your team has tried so you never duplicate work.\\n\\nSave and reproduce previously trained models.\\n\\nShare progress and results with your boss and collaborators.\\n\\nCatch regressions and immediately get alerted when performance drops.\\n\\nBenchmark model performance and compare model versions.\\n\\nCreate a collaborative team\\n\\nSign up or log in to your free W&B account.\\n\\nClick Invite Team in the navigation bar.\\n\\nCreate your team and invite collaborators.\\n\\n:::info Note: Only the admin of an organization can create a new team. :::\\n\\nInvite team members']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuff Prompt\n",
    "\n",
    "We'll now take the content of the retrieved documents, stuff them into prompt template along with the query, and pass into an LLM to obtain the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "context = \"\\n\\n\".join([doc for doc in relevant_docs])\n",
    "prompt = PROMPT.format(context=context, question=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use langchain to call openai chat API with the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your Google Gemini API key from: https://aistudio.google.com/app/apikey\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_GEMINI_API_KEY = getpass(\"Paste your Google Gemini API key from: https://aistudio.google.com/app/apikey\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=GOOGLE_GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This content doesn't explicitly state whether reports within public projects can be shared with specific team members. It mainly describes sharing reports via email or magic links and managing report access within team projects. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_GEMINI_API_KEY\n",
    "llm = GoogleGenerativeAI(model=\"models/gemini-1.5-pro\")\n",
    "response = llm.predict(prompt)\n",
    "Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langchain\n",
    "\n",
    "Langchain gives us tools to do this efficiently in few lines of code. Let's do the same using `RetrievalQA` chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Collection' object has no attribute 'as_retriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Collection' object has no attribute 'as_retriever'"
     ]
    }
   ],
   "source": [
    "db.as_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=GoogleGenerativeAI(model=\"models/gemini-1.5-pro\"), \n",
    "                                 chain_type=\"stuff\")\n",
    "result = qa.run(query)\n",
    "\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", retriever=retriever)\n",
    "result = qa.run(query)\n",
    "\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
